{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9720d507",
   "metadata": {},
   "source": [
    "# **1. Installing important dependencies**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install opencv-python matplotlib labelme albumentations tensorflow tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import uuid\n",
    "import cv2\n",
    "import labelme\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import albumentations as alb\n",
    "from tensorflow import keras\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import Input, Conv2D, Dense, GlobalMaxPool2D\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed546d",
   "metadata": {},
   "source": [
    "# **2. Gather and label images**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a92c6d",
   "metadata": {},
   "source": [
    "### **2.1 Collecting images using OpenCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"data\", \"images\")\n",
    "imgs_number = 30\n",
    "\n",
    "cam = cv2.VideoCapture(1)\n",
    "for i in range(imgs_number):\n",
    "    print(\"Capturing image {}\".format(i))\n",
    "    ret, frame = cam.read()\n",
    "    image_name = os.path.join(path, f\"{str(uuid.uuid1())}.jpg\")\n",
    "    cv2.imwrite(image_name, frame)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b0cb7",
   "metadata": {},
   "source": [
    "### **2.2 Labelling images using LabelMe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ee5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08dbd3c",
   "metadata": {},
   "source": [
    "# **3. Review Dataset & Build Image Loading Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c76d35",
   "metadata": {},
   "source": [
    "### **3.1 Loading images into TensorFlow Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading images from the \"data\" directory by including the path of all images\n",
    "images = tf.data.Dataset.list_files('data/images/*.jpg', shuffle=False)\n",
    "\n",
    "# images.as_numpy_iterator().next() <---------- Testing if the images are loaded\n",
    "\n",
    "# This function will take in the path of the images, turn them into bytes and decode them.\n",
    "def load_images(x):\n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img\n",
    "\n",
    "# Load the images in the function using \"map\"\n",
    "# map: This transformation applies map_func to each element of this dataset, and returns a new dataset containing the transformed elements\n",
    "images = images.map(load_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa85f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47b3c7",
   "metadata": {},
   "source": [
    "### **3.2 View raw images using MatPlotLib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1883f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the images in a batch of 4\n",
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the next batch\n",
    "plot_images = image_generator.next()\n",
    "\n",
    "# Visualize the images\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c072a3d",
   "metadata": {},
   "source": [
    "# **4. Partition Unaugmented Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9e165",
   "metadata": {},
   "source": [
    "### **4.1 Split the data into Train, Test, and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cda265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories \"Train, Test, Val\" with their subdirectories \"Images, and Labels\"\n",
    "Folders = ['Train', 'Test', 'Val']\n",
    "path = 'path/to/data'\n",
    "for i in Folders:\n",
    "    folders_path = os.path.join(path, i)\n",
    "    if not os.path.exists(folders_path):\n",
    "        os.mkdir(folders_path)\n",
    "    if not os.path.exists(os.path.join(folders_path, \"images\")):\n",
    "        os.mkdir(os.path.join(folders_path, \"images\"))\n",
    "    if not os.path.exists(os.path.join(folders_path, \"labels\")):\n",
    "        os.mkdir(os.path.join(folders_path, \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb670e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split images and labels into the different directories\n",
    "image_list, label_list = [], []\n",
    "train_perc, test_perc, val_perc = 0.7, 0.15, 0.15\n",
    "image_path = 'path/to/data/images'\n",
    "label_path = 'path/to/data/labels'\n",
    "\n",
    "if os.path.exists(image_path) and os.path.isdir(image_path):\n",
    "    for filename in os.listdir(image_path):\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_list.append(os.path.join(image_path, filename))\n",
    "\n",
    "if os.path.exists(label_path) and os.path.isdir(label_path):\n",
    "    for filename in os.listdir(label_path):\n",
    "        label_list.append(os.path.join(label_path, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images, total_labels = len(image_list), len(label_list)\n",
    "train_imgs_num, train_labels_num = int(total_images * 0.7), int(total_labels * 0.7)\n",
    "test_imgs_num, test_labels_num = int(total_images * 0.15), int(total_labels * 0.15)\n",
    "val_imgs_num, val_labels_num = int(total_images * 0.15), int(total_labels * 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6478b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(image_list)\n",
    "random.shuffle(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ccc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = image_list[:train_imgs_num]\n",
    "test_imgs = image_list[:test_imgs_num]\n",
    "val_imgs = image_list[:val_imgs_num]\n",
    "\n",
    "train_labels = label_list[:train_labels_num]\n",
    "test_labels = label_list[:test_labels_num]\n",
    "val_labels = label_list[:val_labels_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_images(images, source_folder, destination_folder):\n",
    "    for image in images:\n",
    "        shutil.move(os.path.join(source_folder, image), os.path.join(path, destination_folder, 'images', image))\n",
    "\n",
    "def move_labels(labels, source_folder, destination_folder):\n",
    "    for label in labels:\n",
    "        shutil.move(os.path.join(source_folder, label), os.path.join(path, destination_folder, 'labels', label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89edec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "move_images(train_imgs, image_path, 'Train')\n",
    "move_images(test_imgs, image_path, 'Test')\n",
    "move_images(val_imgs, image_path, 'Val')\n",
    "\n",
    "move_labels(train_labels, label_path, 'Train')\n",
    "move_labels(test_labels, label_path, 'Test')\n",
    "move_labels(val_labels, label_path, 'Val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b145ea",
   "metadata": {},
   "source": [
    "# **5. Apply Image Augmentation on Images and Label using Albumentations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f7468",
   "metadata": {},
   "source": [
    "### **5.1 Setup Albumentation Transform Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=450, height=450),\n",
    "                         alb.HorizontalFlip(p=0.5),\n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2),\n",
    "                         alb.RGBShift(p=0.2),\n",
    "                         alb.VerticalFlip(p=0.5)],\n",
    "                         bbox_params=alb.BboxParams(format='albumentations',\n",
    "                                                    label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31c8a7",
   "metadata": {},
   "source": [
    "### **5.2 Testing pipeline with loaded image using OpenCV and JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa121001",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('path/to/data/Train/images/image.jpg')\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d337cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path/to/data/Train/labels/label.json', 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce27ab",
   "metadata": {},
   "source": [
    "### **5.3 Extract the coordinates and Rescale to match image resolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb15ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [0, 0, 0, 0]\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "coords[3] = label['shapes'][0]['points'][1][1]\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfa29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation from VOC Pascal to Albnumentation\n",
    "coords = list(np.divide(coords, [1920, 1080, 1920, 1080]))\n",
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c5c2f",
   "metadata": {},
   "source": [
    "### **5.4 Apply Augmentations and See Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aea2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'],\n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:],[450,450]).astype(int)),\n",
    "              (255, 0, 0), 2)\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17d59f",
   "metadata": {},
   "source": [
    "# **6. Build and Run Augmentation Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16db92a",
   "metadata": {},
   "source": [
    "### **6.1 Running the pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['Train', 'Test', 'Val']\n",
    "for partition in folders:\n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition,'images', image))\n",
    "\n",
    "        \n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "        \n",
    "            coords = [0, 0, 0.00001, 0.00001]\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            \n",
    "            coords = list(np.divide(coords, [1920, 1080, 1920, 1080]))\n",
    "            if coords[0] > coords[2]:\n",
    "                coords[0], coords[2] = coords[2], coords[0]\n",
    "\n",
    "\n",
    "            print(image)\n",
    "            print(f\"Original coordinates: {label['shapes'][0]['points']}\")\n",
    "            print(f\"Adjusted coordinates: {coords}\")\n",
    "\n",
    "            try:\n",
    "                for x in range(60):\n",
    "                    augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                    cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                    annotation = {}\n",
    "                    annotation['image'] = image\n",
    "\n",
    "                    if os.path.exists(label_path):\n",
    "                        if len(augmented['bboxes']) == 0:\n",
    "                            annotation['bbox'] = [0,0,0,0]\n",
    "                            annotation['class'] = 0\n",
    "                        else:\n",
    "                            annotation['bbox'] = augmented['bboxes'][0]\n",
    "                            annotation['class'] = 1\n",
    "                    else:\n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0\n",
    "\n",
    "                    with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                        json.dump(annotation, f)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'],\n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [450,450]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:],[450,450]).astype(int)),\n",
    "              (255, 0, 0), 2)\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d25175",
   "metadata": {},
   "source": [
    "### **6.2 Load augmented images into TensorFlow Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(x):\n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data/Train/images/*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_images)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data/Test/images/*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_images)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data/Val/images/*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_images)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533a105",
   "metadata": {},
   "source": [
    "# **7. Prepare Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc817b",
   "metadata": {},
   "source": [
    "### **7.1 Build Label Loading Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding='utf-8') as f:\n",
    "        label = json.load(f)\n",
    "\n",
    "    return label['class'], label['bbox']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62871fa7",
   "metadata": {},
   "source": [
    "### **7.2 Load labels into TensorFlow Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data/Train/labels/*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b48c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data/Test/labels/*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22026834",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data/Val/labels/*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ed32c",
   "metadata": {},
   "source": [
    "# **8. Combine Labels & Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372aea0",
   "metadata": {},
   "source": [
    "### **8.1 Check Partition lengths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7325ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420603fc",
   "metadata": {},
   "source": [
    "### **8.2 Create Final Datasets (Images + Labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e67ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(3000)\n",
    "train_dataset = train_dataset.batch(10)\n",
    "train_dataset = train_dataset.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test_dataset = test_dataset.shuffle(3000)\n",
    "test_dataset = test_dataset.batch(10)\n",
    "test_dataset = test_dataset.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val_dataset = val_dataset.shuffle(3000)\n",
    "val_dataset = val_dataset.batch(10)\n",
    "val_dataset = val_dataset.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afae48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d29ca",
   "metadata": {},
   "source": [
    "### **8.3 View Images and Annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train_dataset.as_numpy_iterator()\n",
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d62c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4):\n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "\n",
    "    cv2.rectangle(sample_image,\n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120, 120]).astype(int)),\n",
    "                        (255,0,0),2)\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94773536",
   "metadata": {},
   "source": [
    "# **9. Building Deep Learning model using the Functional API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89f5e0",
   "metadata": {},
   "source": [
    "### **9.1 Import Layers and Base Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# This is the base of what all tensorflow models are built from\n",
    "from keras.models import Model\n",
    "\n",
    "# These are several layers used\n",
    "from keras.layers import Input, Conv2D, Dense, GlobalMaxPool2D, Dropout\n",
    "\n",
    "# Neural network built for image classification\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83f400",
   "metadata": {},
   "source": [
    "### **9.2 Download VGG16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c345316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get rid of these top layers cuz vgg16 is a classification model\n",
    "weights = 'path/to/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "vgg = VGG16(include_top=False, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df94ae72",
   "metadata": {},
   "source": [
    "### **9.3 Build Instance of Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d889f4",
   "metadata": {},
   "source": [
    "####  *When it comes to building a neural network, you build an instance as a function. Building a neural network includes multiple inputs and either one output or multiple outputs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebfc1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    # Define an input layer with a shape of (width, height, channels)\n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "\n",
    "    # Create a VGG16 model with pre-trained weights, excluding the top (fully conntected) layers\n",
    "    vgg = VGG16(include_top=False, weights=weights)(input_layer)\n",
    "\n",
    "    # Apply Global max pooling to the output of the VGG16 model\n",
    "    f1 = GlobalMaxPool2D()(vgg)\n",
    "\n",
    "    # Add a Dense layer with 2048 neurons and ReLU Activation\n",
    "    class1 = Dense(2048, activation=\"relu\", kernel_regularizer=l2(0.001))(f1)\n",
    "    class1 = Dropout(0.5)(class1)\n",
    "    class2 = Dense(1, activation=\"sigmoid\")(class1)\n",
    "\n",
    "\n",
    "    # Apply Global max pooling to the output of the VGG16 model\n",
    "    f2 = GlobalMaxPool2D()(vgg)\n",
    "\n",
    "    # Add a Desne layer with 2048 neurons and ReLU Activation\n",
    "    regress1 = Dense(2048, activation=\"relu\", kernel_regularizer=l2(0.001))(f2)\n",
    "    regress1 = Dropout(0.5)(regress1)\n",
    "    # Add a Dense layer with 4 neurons and Sigmoid Activation (Regression)\n",
    "    regress2 = Dense(4, activation=\"sigmoid\")(regress1)\n",
    "\n",
    "    # Create model with the input and output layers from classification and regression branches\n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edacbd23",
   "metadata": {},
   "source": [
    "### **9.4 Test out Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3640501",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee299921",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = facetracker.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75688187",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b4c9b",
   "metadata": {},
   "source": [
    "# **10. Building Loss Function & Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a95899",
   "metadata": {},
   "source": [
    "#### **The loss function** *(or objective function or cost function) quantifies how well the model is performing on a particular task. It measures the difference between the predicted output and the true target values. During training, the goal is to minimize this loss. In other words, the model aims to find the set of parameters (weights and biases) that result in the smallest possible loss.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff355b2",
   "metadata": {},
   "source": [
    "#### **The optimizer** *is an algorithm that adjusts the model's parameters during training to minimize the loss function. It's responsible for updating the weights and biases based on the gradients of the loss with respect to those parameters. Optimizers play a crucial role in the training process. They determine how quickly the model learns, how it responds to gradients, and how it avoids getting stuck in local minima.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b50e76",
   "metadata": {},
   "source": [
    "### **10.1 Define Optimizer and LR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2de306",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train_dataset)\n",
    "LR_decay = (1/0.75 - 1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92307065",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.legacy.Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a151fb",
   "metadata": {},
   "source": [
    "### **10.2 Creating Localization Loss & Classification Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import MeanSquaredError\n",
    "def localization_loss(y_true, y_pred):\n",
    "    # Use Mean Squared Error loss for regression\n",
    "    mse_loss = MeanSquaredError()(y_true, y_pred)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea995321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are defining the loss funcitons for both classification and regression models\n",
    "\n",
    "class_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81591032",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1],coords).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss(y[0], classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204d83e",
   "metadata": {},
   "source": [
    "# **11. Train Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270782e2",
   "metadata": {},
   "source": [
    "### **10.1 Create Custom Model Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model):\n",
    "    def __init__(self, facetracker, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = facetracker\n",
    "    \n",
    "    def compile(self, opt, class_loss, localization_loss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = class_loss\n",
    "        self.lloss = localization_loss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords = self.model(X, training=True)\n",
    "\n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "\n",
    "            total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "\n",
    "        return{\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "\n",
    "        classes, coords = self.model(X, training=True)\n",
    "\n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "\n",
    "        return{\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "\n",
    "    def call(self, X, **kwargs):\n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0332ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753650b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, class_loss, regressloss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ccf7b",
   "metadata": {},
   "source": [
    "### **11.2 Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_dataset, epochs=30, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704bd5d",
   "metadata": {},
   "source": [
    "# **12. Make Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41115204",
   "metadata": {},
   "source": [
    "### **12.1 Make Predictions on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8dcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facetracker = tf.keras.models.load_model('path/to/facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b1141",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437eb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4):\n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "\n",
    "    if yhat[0][idx] > 0.5:\n",
    "        cv2.rectangle(sample_image,\n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)),\n",
    "                      (255, 0, 0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abdf3e8",
   "metadata": {},
   "source": [
    "### **12.2 Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f99c18",
   "metadata": {},
   "source": [
    "### **12.3 Real Time Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(1)\n",
    "while cam.isOpened():\n",
    "    ret, frame = cam.read()\n",
    "    frame2 = frame[50:500, 50:500,:]\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resize = tf.image.resize(rgb, (120,120))\n",
    "\n",
    "    yhat = facetracker.predict(np.expand_dims(resize/255, 0))\n",
    "    sample_coords = yhat[1][0]\n",
    "\n",
    "    if yhat[0] > 0.5:\n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame2,\n",
    "                      tuple(np.multiply(sample_coords[:2], [450, 450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450, 450]).astype(int)),\n",
    "                      (255, 0, 0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame2, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450, 450]).astype(int),\n",
    "                                   [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450, 450]).astype(int),\n",
    "                                   [80, 0])),\n",
    "                                   (255, 0, 0), -1)\n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame2, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                                [0, -5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow(\"Face Tracker\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e25ce",
   "metadata": {},
   "source": [
    "## THIS IS FOR TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a72184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming 'facetracker' is your loaded model\n",
    "# facetracker = load_model('/path/to/saved_model_directory')\n",
    "\n",
    "cam = cv2.VideoCapture(1)\n",
    "\n",
    "while cam.isOpened():\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # Resize the frame for prediction\n",
    "    resized_frame = cv2.resize(frame, (120, 120))\n",
    "\n",
    "    # Preprocess the frame for prediction\n",
    "    rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "    resize = tf.image.resize(rgb, (120, 120))\n",
    "\n",
    "    # Make predictions\n",
    "    yhat = facetracker.predict(np.expand_dims(resize / 255, 0))\n",
    "    sample_coords = yhat[1][0]\n",
    "\n",
    "    if yhat[0] > 0.5:\n",
    "        # Scale coordinates to the original frame size\n",
    "        x1, y1, x2, y2 = np.round(sample_coords * frame.shape[1]).astype(int)\n",
    "\n",
    "        # Ensure the rectangle stays within the frame bounds\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "\n",
    "        # Draw the main rectangle\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "        # Controls the label rectangle\n",
    "        label_rect_height = 30  # Adjust as needed\n",
    "        cv2.rectangle(frame, (x1, y1 - label_rect_height), (x1 + 80, y1), (255, 0, 0), -1)\n",
    "\n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Face Tracker\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
